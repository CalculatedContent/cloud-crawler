TODO:  need tasks manager again

Release 0.3  social media search
-instagram
-twitter
-vine
-facebook

1.  data store -> PageStore, JsonStore

2.  batch job -> BatchSocialJob
  
3.  test recurring jobs, other qless jobs as batch job
   can we schedule a recurring job
   other features?  do we need timeouts
   
4.  refactor jobs to implement get data, put in store
    batch data job
      gets data
      check bloomfilter to see if data is there
      place in local redis store
      sync local cache to master
      saves to s3
      flush
      notify someone data is ready
      

5.  assign ids to jobs so redis cache can be accessed by id
    ids need some kind of security / authentication
    
6.  set up rack streaming and/or pull endpoint on master
    to allow external access to cache
    
    http://www.intridea.com/blog/2012/5/24/building-streaming-rest-apis-with-ruby
    
    what port?  
    how authenticate?
    also allow simple redis polling
    
    
7.  implement dsl for social
  social_sear
  
  
8.  change DSL to just say crawl
  remove term batch , or at least deprecate , replace with sugar
  social_crawl(opts)
    by default, crawls for hashtags, gets jsons, checks for repeats, syncs to master
    crawls N social media sites and aggregates
    place data in cache for user to retrieve -> sync cache idea
    
    






TODO for release 0.2

A. Implement before and after batch crawl method hooks

  after crawl needs to periodically poll qless and see if all jobs have completed
  if completed, it can do something
   
  needs to run in background as nice , or as a job that is submitted locally
   or as a cronjob>
   
A.*  maybe not 0.2 release
   require some thought

B.  can i actually crawl bodybuilding.com
  get some performance stats on
  b.1  how many jobs can be submitted on small until memory runs out
  b.2  timing statistics and cost estimates for a large crawl,


C. common crawl processor
   c.1 advertise and explain batch_curl, maybe rename to common_crawl (YES)
   c.2 add helper methods for crawling and processing common_crawl data
       
D.  port latest version of QLess
  d.1  try to migrate to latest version and check for bugs

  
E.  net/http vs httparty
   test and see if net/http is actually faster
   

F.  google crawls
  make sure the google crawls actually work
  f.1  crawl for related queries
  f.2  crawl for serps
  
  have some help scripts, that are mine, that support this
  
  
G.  blog psot on DSL
  make sure DSL is described in post
  have some sample scripts in github
  test w/ruby 2.0
  
  
H. Lua Bloom Filter .. instead of bloomd !!!
    should make life easier, not sure if better
    
    
I. Checkpointing:
   add script to pull data from checkpoints and resubit N jobs
   need a simple bookkeeping scheme to make sure we can checkpoint at scale
   really this is silly...just a stopgap...but it is important
  
J. update the deck to 0.2

K.  Not just in domain...under domain
  bodybuilding.com/fun
  
    => has to be under this
    => redirect might be a problem
    
    select_links_below_root
    select_links_in_domain
    select_links_outside_domain
    select_links in_domain and without_queries
    
     
    
    
  L.  make sure chef is not going nuts and retsrating the workers all the time
    really learn chef
    learn ansible
   
-------------------------------------------------

Basic Issues TODO  for / with Pierre and Dave

1.   Make sure INSTALL.local is upto date

1.a  do the examples all run properly AS IS ?

1.b  does rake work?  do all the unit tests work?  
  probably not--but this is an excellent place to start


2.  Create a GIT Tag
  
2.a get the unit tests stabilized and all existing functionality locked
  down.  

2.b create a GIT tag with Gemfile.lock that manages the depenencies


3.  Create INSTALL.aws   instructions
  
3.a   set up ENV VARs  for   AWS

AWS_SECRET_ACCESS_KEY~
AWS_ACCESS_KEY_ID
EC2_AVAILABILITY_ZONE
EC2_REGION

and CHEF:

CHEF_AMI_ID=ami-d70c2892  (unbuntu small on EC2_REGION=us-west-1 )
CHEF_USER=charlesmartin14
CHEF_PEM=/Users/charlesmartin14/.ssh/cms_aws_3.pem
CHEF_KEY=cms_aws_3


3.b create CHEF_KEY , S3 bucket with proper permissions
same as we did for  Katie

3.b  run

 deploy_chef_server.sh

  make sure knife.rb is correct

3.c run 

  deploy_cloud_master.sh

  deploy_cloud_worker.sh




4.  add support for JSON
   jsons store, not page store?
    store raw html, xml, or json in long term storage
    crawl => page is an json doc, so might need different call
    api_crawl => different dsl, only json, and not follow links
      expose json directly
      expose common apis with translation table if possible
      crunchbase.search
      crunchbase.xxx
      
      these can be 'dsl plugins' for each api
      
5.  make a real buglist ... set up bugzilla or list github issues

6.  make sure crawler still works for recipies
    set up large recipe crawls and run  
  
